{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of DQN training progress in the Pendulum environment\n",
    "\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Read the log file\n",
    "\n",
    "\n",
    "with open(r\"..\\logs\\pendulum\\dqn.log\", \"r\") as f:\n",
    "    log_content = f.read()\n",
    "\n",
    "\n",
    "# Extract episode rewards using regex\n",
    "\n",
    "\n",
    "reward_pattern = r\"Episode (\\d+)/\\d+: Total Reward: ([-\\d\\.]+)\"\n",
    "reward_matches = re.findall(reward_pattern, log_content)\n",
    "\n",
    "\n",
    "# Extract epsilon values\n",
    "\n",
    "\n",
    "epsilon_pattern = r\"Episode \\d+/\\d+:.*Epsilon: ([\\d\\.]+)\"\n",
    "epsilon_matches = re.findall(epsilon_pattern, log_content)\n",
    "\n",
    "\n",
    "# Parse the data\n",
    "episodes = [int(match[0]) for match in reward_matches]\n",
    "rewards = [float(match[1]) for match in reward_matches]\n",
    "epsilons = [float(eps) for eps in epsilon_matches]\n",
    "\n",
    "\n",
    "# Create the figure with two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "\n",
    "# Plot rewards on the left y-axis\n",
    "\n",
    "\n",
    "color = \"tab:blue\"\n",
    "ax1.set_xlabel(\"Training Episodes\", fontsize=12)\n",
    "ax1.set_ylabel(\"Total Reward\", fontsize=12)\n",
    "ax1.plot(episodes, rewards, \"o-\", color=color, markersize=3, alpha=0.7, label=\"Episode Reward\")\n",
    "\n",
    "\n",
    "# Add smoothed line (moving average)\n",
    "window_size = 10\n",
    "smoothed_rewards = np.convolve(rewards, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "ax1.plot(\n",
    "    episodes[window_size - 1 :], smoothed_rewards, \"r-\", linewidth=2, label=f\"Moving Average (window={window_size})\"\n",
    ")\n",
    "\n",
    "\n",
    "ax1.tick_params(axis=\"y\")\n",
    "\n",
    "\n",
    "# Create a second y-axis for epsilon\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "\n",
    "color = \"tab:green\"\n",
    "ax2.set_ylabel(\"Exploration Rate (Epsilon)\", fontsize=12)\n",
    "\n",
    "\n",
    "ax2.plot(episodes, epsilons, \"--\", color=color, linewidth=2, label=\"Epsilon\")\n",
    "ax2.tick_params(axis=\"y\")\n",
    "ax2.set_ylim(0, 1.0)  # Set reasonable limits for epsilon\n",
    "\n",
    "\n",
    "# Add a title\n",
    "\n",
    "\n",
    "plt.title(\"DQN Training Progress in Pendulum Environment\", fontsize=14)\n",
    "\n",
    "\n",
    "# Add grid\n",
    "\n",
    "\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "\n",
    "# Mark checkpoint episodes\n",
    "\n",
    "\n",
    "checkpoints = [50, 100, 150, 200]\n",
    "for checkpoint in checkpoints:\n",
    "    ax1.axvline(x=checkpoint, color=\"brown\", linestyle=\"--\", alpha=0.5)\n",
    "    ax1.text(checkpoint + 1, 0, f\"Checkpoint {checkpoint}\", rotation=90, alpha=0.7)\n",
    "\n",
    "\n",
    "# Annotate key points\n",
    "\n",
    "\n",
    "best_reward_idx = np.argmax(rewards)\n",
    "ax1.annotate(\n",
    "    f\"Best Reward: {rewards[best_reward_idx]:.2f}\",\n",
    "    xy=(episodes[best_reward_idx], rewards[best_reward_idx]),\n",
    "    xytext=(episodes[best_reward_idx] + 5, rewards[best_reward_idx] - 60),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"blue\"),\n",
    "    color=\"blue\",\n",
    "    fontsize=10,\n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7),\n",
    ")\n",
    "\n",
    "\n",
    "# Annotate specific epsilon values (0.1, 0.01, 0.001, 0.0001)\n",
    "epsilon_targets = [0.1, 0.01, 0.001, 0.0001]\n",
    "for eps_value in epsilon_targets:\n",
    "    # Find the closest epsilon value and its index\n",
    "\n",
    "    eps_idx = min(range(len(epsilons)), key=lambda i: abs(epsilons[i] - eps_value))\n",
    "\n",
    "    # Add annotation if the epsilon value is close enough\n",
    "    if abs(epsilons[eps_idx] - eps_value) < 0.05:  # Tolerance threshold\n",
    "        ax2.annotate(\n",
    "            f\"Îµ = {eps_value}\" + (\"(min)\" if eps_value == 0.0001 else \"\"),\n",
    "            xy=(episodes[eps_idx], epsilons[eps_idx]),\n",
    "            xytext=(episodes[eps_idx] + 10, epsilons[eps_idx] + 0.1),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"green\", alpha=0.7),\n",
    "            color=\"green\",\n",
    "            fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7),\n",
    "        )\n",
    "\n",
    "\n",
    "# Add legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\", frameon=True, framealpha=0.9)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original reward\n",
    "# reward = -5 * np.power(self.alpha, 2) - 0.1 * np.power(self.dot_alpha, 2) - np.power(action, 2)\n",
    "\n",
    "# improved reward\n",
    "# upright_angle = -np.abs(self.alpha) + np.pi / 2\n",
    "# reward = 0.05 * (\n",
    "#     upright_angle + 0.001 * np.exp(upright_angle * 5)\n",
    "# )  # higher value like 0.2 or lower value like 0.001 might be hard for DQN to learn\n",
    "\n",
    "# if np.abs(np.degrees(self.alpha)) < 20:\n",
    "#     velocity_penalty = 0.05 * (np.tanh(np.abs(self.dot_alpha) * 0.1))\n",
    "#     action_penalty = 0.05 * (np.tanh(np.abs(action) * 0.1))\n",
    "#     reward += 0.1 - velocity_penalty - action_penalty  # Extra reward approaches 0 as velocity increases\n",
    "\n",
    "# action: [-3, 0, 3], alpha: [-pi, pi], dot_alpha: [-15pi, 15pi]  # Updated action range\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define the domain ranges\n",
    "alpha_range = np.linspace(-np.pi, np.pi, 100)\n",
    "dot_alpha_range = np.linspace(-15 * np.pi, 15 * np.pi, 100)\n",
    "actions = [-3, 0, 3]\n",
    "\n",
    "# Create meshgrid for 3D plotting\n",
    "alpha_mesh, dot_alpha_mesh = np.meshgrid(alpha_range, dot_alpha_range)\n",
    "\n",
    "\n",
    "# Define original reward function\n",
    "def original_reward(alpha, dot_alpha, action):\n",
    "    return -5 * np.power(alpha, 2) - 0.1 * np.power(dot_alpha, 2) - np.power(action, 2)\n",
    "\n",
    "\n",
    "# Define improved reward function\n",
    "def improved_reward(alpha, dot_alpha, action):\n",
    "    upright_angle = -np.abs(alpha) + np.pi / 2\n",
    "    reward = 0.05 * (upright_angle + 0.001 * np.exp(upright_angle * 5))\n",
    "\n",
    "    # Add extra reward for angles within 20 degrees of vertical\n",
    "    angle_deg = np.abs(np.degrees(alpha))\n",
    "    mask = angle_deg < 20\n",
    "\n",
    "    velocity_penalty = 0.05 * (np.tanh(np.abs(dot_alpha) * 0.1))\n",
    "    action_penalty = 0.05 * (np.tanh(np.abs(action) * 0.1))\n",
    "\n",
    "    extra_reward = np.zeros_like(alpha)\n",
    "    extra_reward[mask] = 0.1 - velocity_penalty[mask] - action_penalty\n",
    "\n",
    "    return reward + extra_reward\n",
    "\n",
    "\n",
    "# Create a single figure with two rows for both reward functions\n",
    "fig = plt.figure(figsize=(18, 9))\n",
    "\n",
    "# Add subplot for original reward in top row\n",
    "fig.suptitle(\"Reward Functions Comparison\", fontsize=16)\n",
    "\n",
    "# Plot original reward for each action in top row\n",
    "for i, action in enumerate(actions):\n",
    "    # Calculate reward values for current action\n",
    "    reward_values = np.zeros_like(alpha_mesh)\n",
    "    for j in range(len(alpha_range)):\n",
    "        for k in range(len(dot_alpha_range)):\n",
    "            reward_values[k, j] = original_reward(alpha_mesh[k, j], dot_alpha_mesh[k, j], action)\n",
    "\n",
    "    # Add subplot to top row (original reward)\n",
    "    ax = fig.add_subplot(2, 3, i + 1, projection=\"3d\")\n",
    "    surf = ax.plot_surface(\n",
    "        alpha_mesh, dot_alpha_mesh, reward_values, cmap=cm.coolwarm, linewidth=0, antialiased=True, alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"Alpha (rad)\")\n",
    "    ax.set_ylabel(\"Angular Velocity (rad/s)\")\n",
    "    ax.set_zlabel(\"Reward\")\n",
    "    ax.set_title(f\"Original Reward: Action = {action}\")\n",
    "\n",
    "    # Add colorbar\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.9, aspect=12, pad=0.14)\n",
    "\n",
    "# Plot improved reward for each action in bottom row\n",
    "for i, action in enumerate(actions):\n",
    "    # Calculate reward values for current action\n",
    "    reward_values = np.zeros_like(alpha_mesh)\n",
    "    for j in range(len(alpha_range)):\n",
    "        for k in range(len(dot_alpha_range)):\n",
    "            reward_values[k, j] = improved_reward(alpha_mesh[k, j], dot_alpha_mesh[k, j], action)\n",
    "\n",
    "    # Add subplot to bottom row (improved reward)\n",
    "    ax = fig.add_subplot(2, 3, i + 4, projection=\"3d\")\n",
    "    surf = ax.plot_surface(\n",
    "        alpha_mesh, dot_alpha_mesh, reward_values, cmap=cm.viridis, linewidth=0, antialiased=True, alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"Alpha (rad)\")\n",
    "    ax.set_ylabel(\"Angular Velocity (rad/s)\")\n",
    "    ax.set_zlabel(\"Reward\")\n",
    "    ax.set_title(f\"Improved Reward: Action = {action}\")\n",
    "\n",
    "    # Add colorbar\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.9, aspect=12, pad=0.14)\n",
    "\n",
    "# Adjust layout\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of DQN training progress in the Curling environment\n",
    "\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Read the log file\n",
    "with open(r\"..\\logs\\curling\\dqn.log\", \"r\") as f:\n",
    "    log_content = f.read()\n",
    "\n",
    "\n",
    "# Extract episode rewards using regex\n",
    "reward_pattern = r\"Episode (\\d+)/\\d+: Total Reward: ([-\\d\\.]+)\"\n",
    "reward_matches = re.findall(reward_pattern, log_content)\n",
    "\n",
    "\n",
    "# Extract epsilon values\n",
    "epsilon_pattern = r\"Episode \\d+/\\d+:.*Epsilon: ([\\d\\.]+)\"\n",
    "epsilon_matches = re.findall(epsilon_pattern, log_content)\n",
    "\n",
    "\n",
    "# Parse the data\n",
    "episodes = [int(match[0]) for match in reward_matches]\n",
    "rewards = [float(match[1]) for match in reward_matches]\n",
    "epsilons = [float(eps) for eps in epsilon_matches]\n",
    "\n",
    "\n",
    "# Create the figure with two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "\n",
    "# Plot rewards on the left y-axis\n",
    "color = \"tab:blue\"\n",
    "ax1.set_xlabel(\"Training Episodes\", fontsize=12)\n",
    "ax1.set_ylabel(\"Total Reward\", fontsize=12)\n",
    "ax1.plot(episodes, rewards, \"o-\", color=color, markersize=3, alpha=0.7, label=\"Episode Reward\")\n",
    "\n",
    "\n",
    "# Add smoothed line (moving average)\n",
    "window_size = 10\n",
    "smoothed_rewards = np.convolve(rewards, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "ax1.plot(\n",
    "    episodes[window_size - 1 :], smoothed_rewards, \"r-\", linewidth=2, label=f\"Moving Average (window={window_size})\"\n",
    ")\n",
    "\n",
    "\n",
    "ax1.tick_params(axis=\"y\")\n",
    "# Create a second y-axis for epsilon\n",
    "ax2 = ax1.twinx()\n",
    "color = \"tab:green\"\n",
    "ax2.set_ylabel(\"Exploration Rate (Epsilon)\", fontsize=12)\n",
    "ax2.plot(episodes, epsilons, \"--\", color=color, linewidth=2, label=\"Epsilon\")\n",
    "ax2.tick_params(axis=\"y\")\n",
    "ax2.set_ylim(0, 1.0)  # Set reasonable limits for epsilon\n",
    "\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"DQN Training Progress in Curling Environment\", fontsize=14)\n",
    "\n",
    "\n",
    "# Add grid\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "\n",
    "# Mark checkpoint episodes\n",
    "checkpoints = [50, 100, 150, 200]\n",
    "for checkpoint in checkpoints:\n",
    "    ax1.axvline(x=checkpoint, color=\"brown\", linestyle=\"--\", alpha=0.5)\n",
    "    ax1.text(checkpoint + 1, -20000, f\"Checkpoint {checkpoint}\", rotation=90, alpha=0.7)\n",
    "\n",
    "\n",
    "# Annotate key points\n",
    "\n",
    "\n",
    "best_reward_idx = np.argmax(rewards)\n",
    "ax1.annotate(\n",
    "    f\"Best Reward: {rewards[best_reward_idx]:.2f}\",\n",
    "    xy=(episodes[best_reward_idx], rewards[best_reward_idx]),\n",
    "    xytext=(episodes[best_reward_idx] + 5, rewards[best_reward_idx] - 60),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"blue\"),\n",
    "    color=\"blue\",\n",
    "    fontsize=10,\n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7),\n",
    ")\n",
    "\n",
    "\n",
    "# Annotate specific epsilon values (0.1, 0.01, 0.001, 0.0001)\n",
    "epsilon_targets = [0.1, 0.01]\n",
    "for eps_value in epsilon_targets:\n",
    "    # Find the closest epsilon value and its index\n",
    "\n",
    "    eps_idx = min(range(len(epsilons)), key=lambda i: abs(epsilons[i] - eps_value))\n",
    "\n",
    "    # Add annotation if the epsilon value is close enough\n",
    "    if abs(epsilons[eps_idx] - eps_value) < 0.05:  # Tolerance threshold\n",
    "        ax2.annotate(\n",
    "            f\"Îµ = {eps_value}\" + (\"(min)\" if eps_value == 0.01 else \"\"),\n",
    "            xy=(episodes[eps_idx], epsilons[eps_idx]),\n",
    "            xytext=(episodes[eps_idx] + 10, epsilons[eps_idx] + 0.1),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"green\", alpha=0.7),\n",
    "            color=\"green\",\n",
    "            fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7),\n",
    "        )\n",
    "\n",
    "\n",
    "# Add legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\", frameon=True, framealpha=0.9)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of PPO training progress in the Pendulum environment\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read the log file\n",
    "with open(r\"..\\logs\\pendulum\\ppo.log\", \"r\") as f:\n",
    "    log_content = f.read()\n",
    "\n",
    "# Extract episode rewards using regex\n",
    "reward_pattern = r\"Episode (\\d+)/\\d+: Total Reward: ([-\\d\\.]+)\"\n",
    "reward_matches = re.findall(reward_pattern, log_content)\n",
    "\n",
    "# Extract batch update information (for additional analysis)\n",
    "# update_pattern = r\"Update after (\\d+) steps \\((\\d+) episodes\\): Avg Episode Reward: ([-\\d\\.]+), Loss: ([-\\d\\.]+)\"\n",
    "# update_matches = re.findall(update_pattern, log_content)\n",
    "\n",
    "# Parse the data\n",
    "episodes = [int(match[0]) for match in reward_matches]\n",
    "rewards = [float(match[1]) for match in reward_matches]\n",
    "\n",
    "# update_steps = [int(match[0]) for match in update_matches]\n",
    "# update_episodes = [int(match[1]) for match in update_matches]\n",
    "# update_rewards = [float(match[2]) for match in update_matches]\n",
    "# update_losses = [float(match[3]) for match in update_matches]\n",
    "\n",
    "# Create the figure\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot rewards\n",
    "color = \"tab:blue\"\n",
    "ax1.set_xlabel(\"Training Episodes\", fontsize=12)\n",
    "ax1.set_ylabel(\"Total Reward\", fontsize=12)\n",
    "ax1.plot(episodes, rewards, \"o-\", color=color, markersize=3, alpha=0.7, label=\"Episode Reward\")\n",
    "\n",
    "# Add smoothed line (moving average)\n",
    "window_size = 10\n",
    "if len(rewards) > window_size:\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "    ax1.plot(\n",
    "        episodes[window_size - 1 :], smoothed_rewards, \"r-\", linewidth=2, label=f\"Moving Average (window={window_size})\"\n",
    "    )\n",
    "ax1.tick_params(axis=\"y\")\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"PPO Training Progress in Pendulum Environment\", fontsize=14)\n",
    "\n",
    "# Add grid\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Mark checkpoint episodes\n",
    "checkpoints = range(50, 601, 50)  # Example checkpoints\n",
    "for checkpoint in checkpoints:\n",
    "    if checkpoint <= max(episodes):\n",
    "        ax1.axvline(x=checkpoint, color=\"brown\", linestyle=\"--\", alpha=0.5)\n",
    "        ax1.text(checkpoint + 1, min(rewards) - 1, f\"Checkpoint {checkpoint}\", rotation=90, alpha=0.7)\n",
    "\n",
    "# Annotate key points\n",
    "best_reward_idx = np.argmax(rewards)\n",
    "ax1.annotate(\n",
    "    f\"Best Reward: {rewards[best_reward_idx]:.2f}\",\n",
    "    xy=(episodes[best_reward_idx], rewards[best_reward_idx]),\n",
    "    xytext=(episodes[best_reward_idx] - 50, rewards[best_reward_idx] + 5),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"blue\"),\n",
    "    color=\"blue\",\n",
    "    fontsize=10,\n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7),\n",
    ")\n",
    "\n",
    "\n",
    "# Add legend\n",
    "ax1.legend(loc=\"upper left\", frameon=True, framealpha=0.9)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Create a second plot for loss values\n",
    "# fig2, ax2 = plt.subplots(figsize=(14, 5))\n",
    "# ax2.plot(range(len(update_losses)), update_losses, \"o-\", color=\"purple\", markersize=3, alpha=0.7)\n",
    "# ax2.set_xlabel(\"Update Steps\", fontsize=12)\n",
    "# ax2.set_ylabel(\"Loss\", fontsize=12)\n",
    "# ax2.set_title(\"PPO Loss During Training\", fontsize=14)\n",
    "# ax2.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of PPO training progress in the Curling environment\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read the log file\n",
    "with open(r\"..\\logs\\curling\\ppo.log\", \"r\") as f:\n",
    "    log_content = f.read()\n",
    "\n",
    "# Extract episode rewards using regex\n",
    "reward_pattern = r\"Episode (\\d+)/\\d+: Total Reward: ([-\\d\\.]+)\"\n",
    "reward_matches = re.findall(reward_pattern, log_content)\n",
    "\n",
    "# Extract batch update information (for additional analysis)\n",
    "# update_pattern = r\"Update after (\\d+) steps \\((\\d+) episodes\\): Avg Episode Reward: ([-\\d\\.]+), Loss: ([-\\d\\.]+)\"\n",
    "# update_matches = re.findall(update_pattern, log_content)\n",
    "\n",
    "# Parse the data\n",
    "episodes = [int(match[0]) for match in reward_matches]\n",
    "rewards = [float(match[1]) for match in reward_matches]\n",
    "# unscale the reward\n",
    "rewards = [np.linalg.norm(np.array([100, 100])) * reward for reward in rewards]\n",
    "\n",
    "# update_steps = [int(match[0]) for match in update_matches]\n",
    "# update_episodes = [int(match[1]) for match in update_matches]\n",
    "# update_rewards = [float(match[2]) for match in update_matches]\n",
    "# update_losses = [float(match[3]) for match in update_matches]\n",
    "\n",
    "# Create the figure\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot rewards\n",
    "color = \"tab:blue\"\n",
    "ax1.set_xlabel(\"Training Episodes\", fontsize=12)\n",
    "ax1.set_ylabel(\"Total Reward\", fontsize=12)\n",
    "ax1.plot(episodes, rewards, \"o-\", color=color, markersize=3, alpha=0.7, label=\"Episode Reward\")\n",
    "\n",
    "# Add smoothed line (moving average)\n",
    "window_size = 10\n",
    "if len(rewards) > window_size:\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "    ax1.plot(\n",
    "        episodes[window_size - 1 :], smoothed_rewards, \"r-\", linewidth=2, label=f\"Moving Average (window={window_size})\"\n",
    "    )\n",
    "ax1.tick_params(axis=\"y\")\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"PPO Training Progress in Curling Environment\", fontsize=14)\n",
    "\n",
    "# Add grid\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Mark checkpoint episodes\n",
    "checkpoints = range(50, 801, 50)  # Example checkpoints\n",
    "for checkpoint in checkpoints:\n",
    "    if checkpoint <= max(episodes):\n",
    "        ax1.axvline(x=checkpoint, color=\"brown\", linestyle=\"--\", alpha=0.5)\n",
    "        ax1.text(checkpoint + 1, min(rewards) - 1, f\"Checkpoint {checkpoint}\", rotation=90, alpha=0.7)\n",
    "\n",
    "# Annotate key points\n",
    "best_reward_idx = np.argmax(rewards)\n",
    "ax1.annotate(\n",
    "    f\"Best Reward: {rewards[best_reward_idx]:.2f}\",\n",
    "    xy=(episodes[best_reward_idx], rewards[best_reward_idx]),\n",
    "    xytext=(episodes[best_reward_idx] - 150, rewards[best_reward_idx] + 500),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"blue\"),\n",
    "    color=\"blue\",\n",
    "    fontsize=10,\n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7),\n",
    ")\n",
    "\n",
    "\n",
    "# Add legend\n",
    "ax1.legend(loc=\"upper left\", frameon=True, framealpha=0.9)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Create a second plot for loss values\n",
    "# fig2, ax2 = plt.subplots(figsize=(14, 5))\n",
    "# ax2.plot(range(len(update_losses)), update_losses, \"o-\", color=\"purple\", markersize=3, alpha=0.7)\n",
    "# ax2.set_xlabel(\"Update Steps\", fontsize=12)\n",
    "# ax2.set_ylabel(\"Loss\", fontsize=12)\n",
    "# ax2.set_title(\"PPO Loss During Training\", fontsize=14)\n",
    "# ax2.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
